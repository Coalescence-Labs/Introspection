import { hashString } from "@/lib/utils";
import type { GenerateVariantRequest, GenerateVariantResponse, LLMProvider } from "./types";

/**
 * Stub provider that returns deterministic content based on hash
 * Used for testing pipeline without real API calls
 *
 * Real providers will:
 * - Initialize with API keys from env vars
 * - Call actual LLM APIs (Anthropic, OpenAI, Google, etc.)
 * - Handle rate limiting, retries, errors
 * - Return refined editorial content
 */
export class StubProvider implements LLMProvider {
  name = "stub";

  async generateVariant(request: GenerateVariantRequest): Promise<GenerateVariantResponse> {
    // Deterministic hash based on inputs
    const seed = `${request.questionId}-${request.llm}-${request.speechFriendly}`;
    const hash = hashString(seed);

    // Simulate varying response quality
    const quality = hash % 3;
    const qualityMarkers = ["[Basic]", "[Good]", "[Excellent]"];

    return {
      title: `${qualityMarkers[quality]} ${request.questionText}`,
      fullPrompt: `[STUB GENERATED PROMPT]

Quality Level: ${qualityMarkers[quality]}
Question ID: ${request.questionId}
Target LLM: ${request.llm}
Speech-friendly: ${request.speechFriendly}
Deterministic Hash: ${hash}

This is a stub-generated prompt. In production, this would be:
1. Generated by calling the actual ${request.llm} API
2. Refined based on the specific LLM's strengths and patterns
3. Optimized for the question type and user intent
4. Include speech-friendly instructions if requested

The real implementation will use environment variables:
- ANTHROPIC_API_KEY for Claude
- OPENAI_API_KEY for ChatGPT
- GOOGLE_API_KEY for Gemini
- PERPLEXITY_API_KEY for Perplexity

Current inputs:
Question: "${request.questionText}"
Target: ${request.llm}
Speech-friendly: ${request.speechFriendly}

[END STUB]`,
    };
  }
}
